{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ece2a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris SVM kernel comparison (80/20 split):\n",
      "\n",
      "Linear kernel\n",
      "  Accuracy: 1.000\n",
      "  Precision: 1.000\n",
      "  Recall: 1.000\n",
      "  F1 score: 1.000\n",
      "  Confusion matrix:\n",
      "     [10, 0, 0]\n",
      "     [0, 10, 0]\n",
      "     [0, 0, 10]\n",
      "\n",
      "Polynomial kernel\n",
      "  Accuracy: 0.967\n",
      "  Precision: 0.970\n",
      "  Recall: 0.967\n",
      "  F1 score: 0.967\n",
      "  Confusion matrix:\n",
      "     [10, 0, 0]\n",
      "     [0, 9, 1]\n",
      "     [0, 0, 10]\n",
      "\n",
      "RBF kernel\n",
      "  Accuracy: 0.967\n",
      "  Precision: 0.970\n",
      "  Recall: 0.967\n",
      "  F1 score: 0.967\n",
      "  Confusion matrix:\n",
      "     [10, 0, 0]\n",
      "     [0, 9, 1]\n",
      "     [0, 0, 10]\n",
      "\n",
      "Best kernel: Linear (highest test accuracy of 1.000)\n",
      "Reason:  Iris is mostly linearly separable, so linear kernel already separates the classes well.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score, precision_score, recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.2, stratify=iris.target, random_state=42\n",
    ")\n",
    "\n",
    "kernels = {\n",
    "    \"Linear\": SVC(kernel=\"linear\", random_state=42),\n",
    "    \"Polynomial\": SVC(kernel=\"poly\", degree=3, random_state=42),\n",
    "    \"RBF\": SVC(kernel=\"rbf\", random_state=42),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, clf in kernels.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"f1\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "    }\n",
    "    results[name] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "    }\n",
    "\n",
    "print(\"Iris SVM kernel comparison (80/20 split):\")\n",
    "best_kernel = None\n",
    "best_accuracy = -1.0\n",
    "for name, info in results.items():\n",
    "    metrics = info[\"metrics\"]\n",
    "    print(f\"\\n{name} kernel\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"  F1 score: {metrics['f1']:.3f}\")\n",
    "    print(\"  Confusion matrix:\")\n",
    "    for row in info[\"confusion_matrix\"]:\n",
    "        print(\"    \", row.tolist())\n",
    "    if metrics[\"accuracy\"] > best_accuracy:\n",
    "        best_accuracy = metrics[\"accuracy\"]\n",
    "        best_kernel = name\n",
    "\n",
    "print(f\"\\nBest kernel: {best_kernel} (highest test accuracy of {best_accuracy:.3f})\")\n",
    "if best_kernel == \"Linear\":\n",
    "    reason = \"Iris is mostly linearly separable, so linear kernel already separates the classes well.\"\n",
    "elif best_kernel == \"Polynomial\":\n",
    "    reason = \"Polynomial kernel captures mild curvature in the decision boundary that improves accuracy.\"\n",
    "else:\n",
    "    reason = \"RBF flexibly adapts to non-linear regions, giving better balance across classes when overlap exists.\"\n",
    "print(\"Reason: \", reason)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ff7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer SVM without scaling:\n",
      "  Train accuracy: 0.9186813186813186\n",
      "  Test accuracy: 0.9298245614035088\n",
      "\n",
      "Breast Cancer SVM with StandardScaler:\n",
      "  Train accuracy: 0.9824175824175824\n",
      "  Test accuracy: 0.9824561403508771\n",
      "\n",
      "Feature scaling regularizes distances before RBF acts, reducing dominance by large-magnitude features and yielding more stable accuracy on the unseen split.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, test_size=0.2, stratify=cancer.target, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svc_unscaled = SVC(kernel=\"rbf\", random_state=42, gamma=\"scale\")\n",
    "svc_unscaled.fit(X_train, y_train)\n",
    "\n",
    "svc_scaled = SVC(kernel=\"rbf\", random_state=42, gamma=\"scale\")\n",
    "svc_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Breast Cancer SVM without scaling:\")\n",
    "print(\"  Train accuracy:\", accuracy_score(y_train, svc_unscaled.predict(X_train)))\n",
    "print(\"  Test accuracy:\", accuracy_score(y_test, svc_unscaled.predict(X_test)))\n",
    "print(\"\\nBreast Cancer SVM with StandardScaler:\")\n",
    "print(\"  Train accuracy:\", accuracy_score(y_train, svc_scaled.predict(X_train_scaled)))\n",
    "print(\"  Test accuracy:\", accuracy_score(y_test, svc_scaled.predict(X_test_scaled)))\n",
    "\n",
    "print(\n",
    "    \"\\nFeature scaling regularizes distances before RBF acts, reducing dominance by large-magnitude features \"\n",
    "    \"and yielding more stable accuracy on the unseen split.\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
